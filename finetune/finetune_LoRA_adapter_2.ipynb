{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scraping & Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 7.5. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import unsloth\n",
    "from datasets import load_dataset\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# Load LLaMA 3 model\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\" \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rag_prompt_template = \"\"\"Below is an user question input, paired with a retrieval context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    contexts     = examples[\"context\"]\n",
    "    outputs      = examples[\"answer_body\"]\n",
    "    texts = []\n",
    "    for input, context, output in zip(inputs, contexts, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = rag_prompt_template.format(input, context, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = load_dataset(\"pacovaldez/pandas-questions\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_strategy='epoch',\n",
    "        metric_for_best_model='loss',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory on CUDA device 0: 14.74 GB\n",
      "Memory allocated on CUDA device 0: 1.13 GB\n",
      "Memory cached on CUDA device 0: 1.15 GB\n",
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "1.148 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def print_cuda_memory_stats(device_index=0):\n",
    "    # Ensure CUDA is available\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available.\")\n",
    "        return\n",
    "    \n",
    "    # Select the device\n",
    "    device = torch.device(f\"cuda:{device_index}\")\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # Get memory statistics\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "    \n",
    "    # Convert bytes to gigabytes for easier interpretation\n",
    "    total_memory_gb = total_memory / (1024 ** 3)\n",
    "    allocated_memory_gb = allocated_memory / (1024 ** 3)\n",
    "    cached_memory_gb = cached_memory / (1024 ** 3)\n",
    "    \n",
    "    # Print memory information\n",
    "    print(f\"Total memory on CUDA device {device_index}: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"Memory allocated on CUDA device {device_index}: {allocated_memory_gb:.2f} GB\")\n",
    "    print(f\"Memory cached on CUDA device {device_index}: {cached_memory_gb:.2f} GB\")\n",
    "\n",
    "print_cuda_memory_stats(0)  # for device index 0\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 7,860 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 16\n",
      "\\        /    Total batch size = 128 | Total steps = 183\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 5:55:16, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.828100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.675700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.643300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.738200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.574300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.550700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.508000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.475200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21433.5543 seconds used for training.\n",
      "357.23 minutes used for training.\n",
      "Peak reserved memory = 13.92 GB.\n",
      "Peak reserved memory for training = 12.772 GB.\n",
      "Peak reserved memory % of max memory = 94.43 %.\n",
      "Peak reserved memory for training % of max memory = 86.643 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an user question input, paired with a retrieval context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Input:\n",
      "Explain how to merge DataFrames in pandas\n",
      "\n",
      "### Context:\n",
      "Pandas merge function can be used to merge two dataframes. This function is similar to SQL join operation.\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the merge function to merge two dataframes. Here is how you can do it.\n",
      "\n",
      "merge(df1, df2, on='id', how='left', left_index=False, left_on=None, right_index=False, right_on=None)\n",
      "\n",
      "\n",
      "Parameters\n",
      "\n",
      "on {str, list, Series, or list of str, list of Series, or Series of str}: The column(s) to use for the merge. If on is a list of columns, it will be used for a left join. If on is a Series, it will be used for a right join. If on is a list of Series, it will be used for an inner join. If on is None, the default is to use the column names of the first DataFrame. If left_index and right_index are both True, then the merge is performed as a left join. If left_index and right_index are both False, then the merge is performed as a right join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join.\n",
      "\n",
      "how {str, 'left', 'right', 'inner', 'outer'}: The method to use for the merge. The default is 'left'.\n",
      "\n",
      "left_index {bool, default False}: If True, the index of the left DataFrame will be used as the index of the right DataFrame. If False, the index of the right DataFrame will be used as the index of the left DataFrame.\n",
      "\n",
      "left_on {str, list, Series, or list of str, list of Series, or Series of str}: The column(s) to use for the left join. If on is a list of columns, it will be used for a left join. If on is a Series, it will be used for a right join. If on is a list of Series, it will be used for an inner join. If on is None, the default is to use the column names of the first DataFrame. If left_index and right_index are both True, then the merge is performed as a left join. If left_index and right_index are both False, then the merge is performed as a right join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join.\n",
      "\n",
      "right_index {bool, default False}: If True, the index of the right DataFrame will be used as the index of the left DataFrame. If False, the index of the left DataFrame will be used as the index of the right DataFrame.\n",
      "\n",
      "right_on {str, list, Series, or list of str, list of Series, or Series of str}: The column(s) to use for the right join. If on is a list of columns, it will be used for a right join. If on is a Series, it will be used for a left join. If on is a list of Series, it will be used for an inner join. If on is None, the default is to use the column names of the first DataFrame. If left_index and right_index are both True, then the merge is performed as a left join. If left_index and right_index are both False, then the merge is performed as a right join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join.\n",
      "\n",
      "See also\n",
      "\n",
      "DataFrame.merge, DataFrame.concat\n",
      "\n",
      "\n",
      "See also\n",
      "\n",
      "DataFrame.merge, DataFrame.concat\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "The merge function is used to combine two dataframes based on some columns. The merge function is similar to the SQL join operation.\n",
      "\n",
      "The merge function can be used for the following types of merge:\n",
      "\n",
      "- Left join: If the left index is True, the left DataFrame will be used as the index of the right DataFrame. If the right index is True, the right DataFrame will be used as the index of the left DataFrame.\n",
      "- Right join: If the left index is True, the right DataFrame will be used as the index of the left DataFrame. If the right index is True, the right DataFrame will be used as the index of the left DataFrame.\n",
      "- Inner join: If the left index is True, the merge will be performed as an inner join. If the right index is True, the merge will be performed as an inner join.\n",
      "- Left outer join: If the left index is True, the merge will be performed as a left outer join. If the right index is True, the merge will be performed as a left outer join.\n",
      "- Right outer join: If the left index is True, the merge will be performed as a right outer join. If the right index is True, the merge will be performed as a right outer join.\n",
      "\n",
      "The merge function can also be used with the following parameters:\n",
      "\n",
      "- left_index: If True, the index of the left DataFrame will be used as the index of the right DataFrame. If False, the index of the right DataFrame will be used as the index of the left DataFrame.\n",
      "- right_index: If True, the index of the right DataFrame will be used as the index of the left DataFrame. If False, the index of the left DataFrame will be used as the index of the right DataFrame.\n",
      "- left_on: The column(s) to use for the left join. If on is a list of columns, it will be used for a left join. If on is a Series, it will be used for a right join. If on is a list of Series, it will be used for an inner join. If on is None, the default is to use the column names of the first DataFrame. If left_index and right_index are both True, then the merge is performed as a left join. If left_index and right_index are both False, then the merge is performed as a right join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join.\n",
      "- right_on: The column(s) to use for the right join. If on is a list of columns, it will be used for a right join. If on is a Series, it will be used for a left join. If on is a list of Series, it will be used for an inner join. If on is None, the default is to use the column names of the first DataFrame. If left_index and right_index are both True, then the merge is performed as a left join. If left_index and right_index are both False, then the merge is performed as a right join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join. If left_index and right_index are both True, then the merge is performed as an inner join. If left_index and right_index are both False, then the merge is performed as a left outer join.\n",
      "- See also DataFrame.merge, DataFrame.concat.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    rag_prompt_template.format(\n",
    "        \"Explain how to merge DataFrames in pandas\", # input\n",
    "        \"Pandas merge function can be used to merge two dataframes. This function is similar to SQL join operation.\", # context\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Llama-3.2-1B-Instruct-Batch-8-Step-183-lr-2e_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'llama_trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"llama_trained_model\")\n",
    "# tokenizer.save_pretrained(\"llama_trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
