{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scraping & Package Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Llama patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 7.5. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:204: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<string>:205: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<string>:206: SyntaxWarning: invalid escape sequence '\\ '\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import unsloth\n",
    "from datasets import load_dataset\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "# Load LLaMA 3 model\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\" \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rag_prompt_template = \"\"\"Below is an user question input, paired with a retrieval context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"question\"]\n",
    "    contexts     = examples[\"context\"]\n",
    "    outputs      = examples[\"answer_body\"]\n",
    "    texts = []\n",
    "    for input, context, output in zip(inputs, contexts, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = rag_prompt_template.format(input, context, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = load_dataset(\"pacovaldez/pandas-questions\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 5, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 1e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        save_strategy='epoch',\n",
    "        metric_for_best_model='loss',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory on CUDA device 0: 14.74 GB\n",
      "Memory allocated on CUDA device 0: 1.13 GB\n",
      "Memory cached on CUDA device 0: 1.15 GB\n",
      "GPU = Tesla T4. Max memory = 14.741 GB.\n",
      "1.148 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def print_cuda_memory_stats(device_index=0):\n",
    "    # Ensure CUDA is available\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available.\")\n",
    "        return\n",
    "    \n",
    "    # Select the device\n",
    "    device = torch.device(f\"cuda:{device_index}\")\n",
    "    torch.cuda.set_device(device)\n",
    "    \n",
    "    # Get memory statistics\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "    \n",
    "    # Convert bytes to gigabytes for easier interpretation\n",
    "    total_memory_gb = total_memory / (1024 ** 3)\n",
    "    allocated_memory_gb = allocated_memory / (1024 ** 3)\n",
    "    cached_memory_gb = cached_memory / (1024 ** 3)\n",
    "    \n",
    "    # Print memory information\n",
    "    print(f\"Total memory on CUDA device {device_index}: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"Memory allocated on CUDA device {device_index}: {allocated_memory_gb:.2f} GB\")\n",
    "    print(f\"Memory cached on CUDA device {device_index}: {cached_memory_gb:.2f} GB\")\n",
    "\n",
    "print_cuda_memory_stats(0)  # for device index 0\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 7,860 | Num Epochs = 5\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 16\n",
      "\\        /    Total batch size = 128 | Total steps = 305\n",
      " \"-____-\"     Number of trainable parameters = 11,272,192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='305' max='305' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [305/305 9:37:56, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.830800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.744000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.584600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.725600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.545800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.528900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.515400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.630900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.510400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.490300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34790.975 seconds used for training.\n",
      "579.85 minutes used for training.\n",
      "Peak reserved memory = 13.92 GB.\n",
      "Peak reserved memory for training = 12.772 GB.\n",
      "Peak reserved memory % of max memory = 94.43 %.\n",
      "Peak reserved memory for training % of max memory = 86.643 %.\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an user question input, paired with a retrieval context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Input:\n",
      "Explain how to merge DataFrames in pandas\n",
      "\n",
      "### Context:\n",
      "Pandas merge function can be used to merge two dataframes. This function is similar to SQL join operation.\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can use the merge function to merge two dataframes. Here's how you can do it.\n",
      "\n",
      "You can use the merge function to merge two dataframes. This function is similar to SQL join operation.\n",
      "\n",
      "Here is an example:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create two dataframes\n",
      "df1 = pd.DataFrame({\n",
      "    'id': [1, 2, 3, 4, 5],\n",
      "    'name': ['A', 'B', 'C', 'D', 'E'],\n",
      "    'age': [20, 21, 19, 20, 22]\n",
      "})\n",
      "\n",
      "df2 = pd.DataFrame({\n",
      "    'id': [1, 2, 3, 4, 5],\n",
      "    'city': ['A', 'B', 'C', 'D', 'E'],\n",
      "    'country': ['USA', 'UK', 'France', 'Canada', 'Germany']\n",
      "})\n",
      "\n",
      "# Merge the two dataframes\n",
      "df = pd.merge(df1, df2, on='id')\n",
      "print(df)\n",
      "\n",
      "# Output:\n",
      "#   id     name  age       city  country\n",
      "# 0  1.0  A.0    20.0     A.0         USA\n",
      "# 1  1.0  B.0    21.0     B.0         UK\n",
      "# 2  1.0  C.0    19.0     C.0         France\n",
      "# 3  1.0  D.0    20.0     D.0         Canada\n",
      "# 4  1.0  E.0    22.0     E.0         Germany\n",
      "\n",
      "# Merge the two dataframes on a specific column\n",
      "df = pd.merge(df1, df2, on='id', left_on='id', right_on='id')\n",
      "print(df)\n",
      "\n",
      "# Output:\n",
      "#   id     name  age       city  country\n",
      "# 0  1.0  A.0    20.0     A.0         USA\n",
      "# 1  1.0  B.0    21.0     B.0         UK\n",
      "# 2  1.0  C.0    19.0     C.0         France\n",
      "# 3  1.0  D.0    20.0     D.0         Canada\n",
      "# 4  1.0  E.0    22.0     E.0         Germany\n",
      "\n",
      "# Merge the two dataframes on a specific column and keep the first occurrence of each id\n",
      "df = pd.merge(df1, df2, on='id', how='left')\n",
      "print(df)\n",
      "\n",
      "# Output:\n",
      "#   id     name  age       city  country\n",
      "# 0  1.0  A.0    20.0     A.0         USA\n",
      "# 1  1.0  B.0    21.0     B.0         UK\n",
      "# 2  1.0  C.0    19.0     C.0         France\n",
      "# 3  1.0  D.0    20.0     D.0         Canada\n",
      "# 4  1.0  E.0    22.0     E.0         Germany\n",
      "# 5  2.0  A.0    20.0     A.0         USA\n",
      "# 6  2.0  B.0    21.0     B.0         UK\n",
      "# 7  2.0  C.0    19.0     C.0         France\n",
      "# 8  2.0  D.0    20.0     D.0         Canada\n",
      "# 9  2.0  E.0    22.0     E.0         Germany\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "In the above code, we use the merge function to merge two dataframes. The merge function can be used to merge two dataframes by specifying the columns to merge on, the join type, and the how to merge.\n",
      "\n",
      "In this code, we use the merge function to merge two dataframes, df1 and df2. We specify the columns to merge on, which is 'id', and the join type, which is 'left'. We also specify the how to merge, which is 'left'.\n",
      "\n",
      "In the output, we see the merged dataframe, which is a combination of the two original dataframes. The merged dataframe has the same columns as the original dataframes, but some of the rows may be missing. The rows that are missing are marked with a '0' in the output.\n",
      "\n",
      "In the second part of the code, we use the merge function with the 'left' join type. We specify the columns to merge on, which is 'id', and the how to merge, which is 'left'. We also specify the how to keep the first occurrence of each id, which is 'left'.\n",
      "\n",
      "In the output, we see the merged dataframe, which is a combination of the two original dataframes. The merged dataframe has the same columns as the original dataframes, but some of the rows may be missing. The rows that are missing are marked with a '0' in the output. We also see the rows that are kept, which are marked with a '1' in the output. The rows that are kept are the first occurrence of each id in the original dataframes.\n",
      "\n",
      "In the last part of the code, we use the merge function with the 'left' join type and the how to keep the first occurrence of each id. We specify the columns to merge on, which is 'id', and the how to keep the first occurrence of each id, which is 'left'.\n",
      "\n",
      "In the output, we see the merged dataframe, which is a combination of the two original dataframes. The merged dataframe has the same columns as the original dataframes, but some of the rows may be missing. The rows that are missing are marked with a '0' in the output. We also see the rows that are kept, which are marked with a '1' in the output. The rows that are kept are the first occurrence of each id in the original dataframes.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    rag_prompt_template.format(\n",
    "        \"Explain how to merge DataFrames in pandas\", # input\n",
    "        \"Pandas merge function can be used to merge two dataframes. This function is similar to SQL join operation.\", # context\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Llama-3.2-1B-Instruct-Batch-8-Step-183-lr-1e_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'llama_trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"llama_trained_model\")\n",
    "# tokenizer.save_pretrained(\"llama_trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
